{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2416d76",
   "metadata": {},
   "source": [
    "# Importer les librairies necessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfbcfe",
   "metadata": {},
   "source": [
    "La librairie PyTorch est la librairie principale de ce projet, elle offre les outils necessaires pour creer des reseaux de neuronnes, les entrainer, et les utiliser. La librairie PyTorch Lightning organise et structure le code PyTorch de sorte a accelerer la creation et l'entrainement des modeles dd'apprentissage profond. Les autres librairies sont des librairies classiques de traitement de donnees (Pandas) et de calcul scientifique (Numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda8cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from data_processing import get_context, pad_list, map_column, MASK, PAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b0b67",
   "metadata": {},
   "source": [
    "# Creation du Modele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c71485",
   "metadata": {},
   "source": [
    "Le modèle qu'on implémente s'appelle BERT4Rec et est basé sur BERT pour le NLP. C'est un réseau de neurones Transformer entraîné pour prédire des films \"masqués\" de l'historique d'un utilisateur. Voici le papier original: https://arxiv.org/pdf/1904.06690.pdf\n",
    "\n",
    "La première étape consiste à construire l'historique de l'utilisateur sous forme de liste chronologique de films. Certains de ces films sont remplacés par un jeton [MASK]. Le modèle BERT4Rec est ensuite entraîné à essayer de prédire les valeurs correctes des éléments [MASK]. En faisant cela, le modèle apprendra des représentations utiles pour chaque film et également des motifs importants qui existent entre les films. Puis, pour l'inférence, nous pouvons simplement ajouter un [MASK] à la fin d'une séquence d'utilisateur pour prédire le film qu'il voudra le plus probablement voir à l'avenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d73bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor, mask: torch.Tensor):\n",
    "\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "\n",
    "    y_true = torch.masked_select(y_true, mask)\n",
    "    predicted = torch.masked_select(predicted, mask)\n",
    "\n",
    "    acc = (y_true == predicted).double().mean()\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def masked_ce(y_pred, y_true, mask):\n",
    "\n",
    "    loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n",
    "\n",
    "    loss = loss * mask\n",
    "\n",
    "    return loss.sum() / (mask.sum() + 1e-8)\n",
    "\n",
    "\n",
    "class Recommender(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        channels=128,\n",
    "        cap=0,\n",
    "        mask=1,\n",
    "        dropout=0.4,\n",
    "        lr=1e-4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cap = cap\n",
    "        self.mask = mask\n",
    "\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.item_embeddings = torch.nn.Embedding(\n",
    "            self.vocab_size, embedding_dim=channels\n",
    "        )\n",
    "\n",
    "        self.input_pos_embedding = torch.nn.Embedding(512, embedding_dim=channels)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels, nhead=4, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "\n",
    "        self.linear_out = Linear(channels, self.vocab_size)\n",
    "\n",
    "        self.do = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def encode_src(self, src_items):\n",
    "        src_items = self.item_embeddings(src_items)\n",
    "\n",
    "        batch_size, in_sequence_len = src_items.size(0), src_items.size(1)\n",
    "        pos_encoder = (\n",
    "            torch.arange(0, in_sequence_len, device=src_items.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "        pos_encoder = self.input_pos_embedding(pos_encoder)\n",
    "\n",
    "        src_items += pos_encoder\n",
    "\n",
    "        src = src_items.permute(1, 0, 2)\n",
    "\n",
    "        src = self.encoder(src)\n",
    "\n",
    "        return src.permute(1, 0, 2)\n",
    "\n",
    "    def forward(self, src_items):\n",
    "\n",
    "        src = self.encode_src(src_items)\n",
    "\n",
    "        out = self.linear_out(src)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src_items, y_true = batch\n",
    "\n",
    "        y_pred = self(src_items)\n",
    "\n",
    "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
    "        y_true = y_true.view(-1)\n",
    "\n",
    "        src_items = src_items.view(-1)\n",
    "        mask = src_items == self.mask\n",
    "\n",
    "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
    "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src_items, y_true = batch\n",
    "\n",
    "        y_pred = self(src_items)\n",
    "\n",
    "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
    "        y_true = y_true.view(-1)\n",
    "\n",
    "        src_items = src_items.view(-1)\n",
    "        mask = src_items == self.mask\n",
    "\n",
    "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
    "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
    "\n",
    "        self.log(\"valid_loss\", loss)\n",
    "        self.log(\"valid_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src_items, y_true = batch\n",
    "\n",
    "        y_pred = self(src_items)\n",
    "\n",
    "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
    "        y_true = y_true.view(-1)\n",
    "\n",
    "        src_items = src_items.view(-1)\n",
    "        mask = src_items == self.mask\n",
    "\n",
    "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
    "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, patience=10, factor=0.1\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"valid_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f2b8f",
   "metadata": {},
   "source": [
    "# Entrainement du Modele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332934f",
   "metadata": {},
   "source": [
    "le modele a ete entraine pendant 100 epochs en utilisant une taille de batch de 64, l'optimiseur Adam avec un pas d'apprentissage de 10e-4, et la fonction de perte Cross Entropy. La taille de l'historique utilise durant l'entrainement est de 120 items (films).\n",
    "\n",
    "Les donnees utilisees proviennent du Dataset MovieLens 1M, qui contient 1 million de notes de 6000 utilisateurs sur 4000 films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24666f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_list(l1, p=0.8):\n",
    "\n",
    "    l1 = [a if random.random() < p else MASK for a in l1]\n",
    "\n",
    "    return l1\n",
    "\n",
    "\n",
    "def mask_last_elements_list(l1, val_context_size: int = 5):\n",
    "\n",
    "    l1 = l1[:-val_context_size] + mask_list(l1[-val_context_size:], p=0.5)\n",
    "\n",
    "    return l1\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, groups, grp_by, split, history_size=120):\n",
    "        self.groups = groups\n",
    "        self.grp_by = grp_by\n",
    "        self.split = split\n",
    "        self.history_size = history_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        group = self.groups[idx]\n",
    "\n",
    "        df = self.grp_by.get_group(group)\n",
    "\n",
    "        context = get_context(df, split=self.split, context_size=self.history_size)\n",
    "\n",
    "        trg_items = context[\"movieId_mapped\"].tolist()\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            src_items = mask_list(trg_items)\n",
    "        else:\n",
    "            src_items = mask_last_elements_list(trg_items)\n",
    "\n",
    "        pad_mode = \"left\" if random.random() < 0.5 else \"right\"\n",
    "        trg_items = pad_list(trg_items, history_size=self.history_size, mode=pad_mode)\n",
    "        src_items = pad_list(src_items, history_size=self.history_size, mode=pad_mode)\n",
    "\n",
    "        src_items = torch.tensor(src_items, dtype=torch.long)\n",
    "\n",
    "        trg_items = torch.tensor(trg_items, dtype=torch.long)\n",
    "\n",
    "        return src_items, trg_items\n",
    "\n",
    "\n",
    "def train(\n",
    "    data_csv_path: str,\n",
    "    log_dir: str = \"recommender_logs\",\n",
    "    model_dir: str = \"recommender_models\",\n",
    "    batch_size: int = 64,\n",
    "    epochs: int = 5,\n",
    "    history_size: int = 120,\n",
    "):\n",
    "    data = pd.read_csv(data_csv_path)\n",
    "\n",
    "    data.sort_values(by=\"timestamp\", inplace=True)\n",
    "\n",
    "    data, mapping, inverse_mapping = map_column(data, col_name=\"movieId\")\n",
    "\n",
    "    grp_by_train = data.groupby(by=\"userId\")\n",
    "\n",
    "    groups = list(grp_by_train.groups)\n",
    "\n",
    "    train_data = Dataset(\n",
    "        groups=groups,\n",
    "        grp_by=grp_by_train,\n",
    "        split=\"train\",\n",
    "        history_size=history_size,\n",
    "    )\n",
    "    val_data = Dataset(\n",
    "        groups=groups,\n",
    "        grp_by=grp_by_train,\n",
    "        split=\"val\",\n",
    "        history_size=history_size,\n",
    "    )\n",
    "\n",
    "    print(\"len(train_data)\", len(train_data))\n",
    "    print(\"len(val_data)\", len(val_data))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=True,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=False,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    model = Recommender(\n",
    "        vocab_size=len(mapping) + 2,\n",
    "        lr=1e-4,\n",
    "        dropout=0.3,\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"valid_loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=model_dir,\n",
    "        filename=\"recommender\",\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    result_val = trainer.test(model, val_loader)\n",
    "\n",
    "    output_json = {\n",
    "        \"val_loss\": result_val[0][\"test_loss\"],\n",
    "        \"best_model_path\": checkpoint_callback.best_model_path,\n",
    "    }\n",
    "\n",
    "    print(output_json)\n",
    "\n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250731ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_csv_path\", type=str, default=\"data/MovieLens 1M Dataset/ml-1m/ratings.csv\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "args = parser.parse_args()\n",
    "\n",
    "train(\n",
    "    data_csv_path=args.data_csv_path,\n",
    "    epochs=args.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd480a",
   "metadata": {},
   "source": [
    "A la fin de l'entrainement, on sauvegarde le modele. Dans notre cas, ce dernier a acheve une perte sur les donnees de test de 6.16 ainsi qu'une precision de 1.11% apres 100 epochs. Dans le papier, le modele a acheve 28% de precision sans mentionner le nombre d'epochs d'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28368e82",
   "metadata": {},
   "source": [
    "# Utilisation du modele entraine pour la recommandation de films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f687dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path = \"data/MovieLens 1M Dataset/ml-1m/ratings.csv\"\n",
    "movies_path = \"data/MovieLens 1M Dataset/ml-1m/movies.csv\"\n",
    "\n",
    "model_path = \"recommender_models/recommender.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cac77364",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_csv_path)\n",
    "movies = pd.read_csv(movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb1a33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"timestamp\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a65c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, mapping, inverse_mapping = map_column(data, col_name=\"movieId\")\n",
    "grp_by_train = data.groupby(by=\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6923b93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1366, 4410, 2109, 3235, 5368, 361, 4867, 3173, 2938, 47]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(grp_by_train.groups), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80c7cc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Recommender(\n",
    "        vocab_size=len(mapping) + 2,\n",
    "        lr=1e-4,\n",
    "        dropout=0.3,\n",
    "    )\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(model_path)[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "997385df",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_to_idx = {a: mapping[b] for a, b in zip(movies.title.tolist(), movies.movieId.tolist()) if b in mapping}\n",
    "idx_to_movie = {v: k for k, v in movie_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b083cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(list_movies, model, movie_to_idx, idx_to_movie):\n",
    "    \n",
    "    ids = [PAD] * (120 - len(list_movies) - 1) + [movie_to_idx[a] for a in list_movies] + [MASK]\n",
    "    \n",
    "    src = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(src)\n",
    "    \n",
    "    masked_pred = prediction[0, -1].numpy()\n",
    "    \n",
    "    sorted_predicted_ids = np.argsort(masked_pred).tolist()[::-1]\n",
    "    \n",
    "    sorted_predicted_ids = [a for a in sorted_predicted_ids if a not in ids]\n",
    "    \n",
    "    return [idx_to_movie[a] for a in sorted_predicted_ids[:30] if a in idx_to_movie]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e44c1",
   "metadata": {},
   "source": [
    "### Senario 1: Aventure/Fantaisie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dae87be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Star Wars: Episode IV - A New Hope (1977)',\n",
       " 'Star Wars: Episode V - The Empire Strikes Back (1980)',\n",
       " 'E.T. the Extra-Terrestrial (1982)',\n",
       " 'Alien (1979)',\n",
       " 'Star Wars: Episode VI - Return of the Jedi (1983)',\n",
       " 'Raiders of the Lost Ark (1981)',\n",
       " 'Back to the Future (1985)',\n",
       " 'Jaws (1975)',\n",
       " 'Jurassic Park (1993)',\n",
       " 'Princess Bride The (1987)',\n",
       " 'Godfather The (1972)',\n",
       " 'American Beauty (1999)',\n",
       " 'Abyss The (1989)',\n",
       " '2001: A Space Odyssey (1968)',\n",
       " 'Ghostbusters (1984)',\n",
       " 'Godfather: Part II The (1974)',\n",
       " 'Terminator 2: Judgment Day (1991)',\n",
       " '20000 Leagues Under the Sea (1954)',\n",
       " 'Braveheart (1995)',\n",
       " 'Willy Wonka and the Chocolate Factory (1971)',\n",
       " 'Saving Private Ryan (1998)',\n",
       " 'Matrix The (1999)',\n",
       " 'Psycho (1960)',\n",
       " 'Goonies The (1985)',\n",
       " 'Indiana Jones and the Last Crusade (1989)',\n",
       " 'Wizard of Oz The (1939)',\n",
       " 'Beetlejuice (1988)',\n",
       " 'Hook (1991)',\n",
       " 'NeverEnding Story The (1984)',\n",
       " \"Schindler's List (1993)\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_movies = [\"Willow (1988)\",\n",
    "               \"Star Wars: Episode I - The Phantom Menace (1999)\",\n",
    "               \"Time Bandits (1981)\",\n",
    "               \"Ladyhawke (1985)\"]\n",
    "\n",
    "top_movie = predict(list_movies, model, movie_to_idx, idx_to_movie)\n",
    "top_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4b657",
   "metadata": {},
   "source": [
    "### Senario 2:  Action/Aventure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96f0c5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Star Wars: Episode IV - A New Hope (1977)',\n",
       " 'Alien (1979)',\n",
       " 'Jaws (1975)',\n",
       " 'Psycho (1960)',\n",
       " 'Star Wars: Episode V - The Empire Strikes Back (1980)',\n",
       " 'Godfather The (1972)',\n",
       " '2001: A Space Odyssey (1968)',\n",
       " 'Godfather: Part II The (1974)',\n",
       " 'King Kong (1933)',\n",
       " 'Matrix The (1999)',\n",
       " 'Terminator 2: Judgment Day (1991)',\n",
       " 'Exorcist The (1973)',\n",
       " 'Raiders of the Lost Ark (1981)',\n",
       " 'Star Wars: Episode VI - Return of the Jedi (1983)',\n",
       " 'American Beauty (1999)',\n",
       " 'Birds The (1963)',\n",
       " 'Ghostbusters (1984)',\n",
       " 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)',\n",
       " 'Star Wars: Episode I - The Phantom Menace (1999)',\n",
       " 'E.T. the Extra-Terrestrial (1982)',\n",
       " 'Halloween (1978)',\n",
       " 'Shining The (1980)',\n",
       " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)',\n",
       " 'Braveheart (1995)',\n",
       " 'Dracula (1931)',\n",
       " 'Blade Runner (1982)',\n",
       " '20000 Leagues Under the Sea (1954)',\n",
       " 'Butch Cassidy and the Sundance Kid (1969)',\n",
       " 'Saving Private Ryan (1998)',\n",
       " 'Good The Bad and The Ugly The (1966)']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_movies = [\"Golden Voyage of Sinbad The (1974)\",\n",
    "               \"Sinbad and the Eye of the Tiger (1977)\",\n",
    "               \"Godzilla 2000 (Gojira ni-sen mireniamu) (1999)\",\n",
    "               \"Mortal Kombat (1995)\",\n",
    "               \"Judge Dredd (1995)\",\n",
    "               \"Waterworld (1995)\",\n",
    "]\n",
    "top_movie = predict(list_movies, model, movie_to_idx, idx_to_movie)\n",
    "top_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ddfeb",
   "metadata": {},
   "source": [
    "### Senario 3: Comedie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3f01cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aladdin (1992)',\n",
       " 'Babe (1995)',\n",
       " 'Shakespeare in Love (1998)',\n",
       " \"Bug's Life A (1998)\",\n",
       " 'Beauty and the Beast (1991)',\n",
       " 'Groundhog Day (1993)',\n",
       " 'American Beauty (1999)',\n",
       " 'Mary Poppins (1964)',\n",
       " 'Lion King The (1994)',\n",
       " 'Princess Bride The (1987)',\n",
       " 'Little Mermaid The (1989)',\n",
       " 'Lady and the Tramp (1955)',\n",
       " '101 Dalmatians (1961)',\n",
       " 'Being John Malkovich (1999)',\n",
       " 'Snow White and the Seven Dwarfs (1937)',\n",
       " 'Hercules (1997)',\n",
       " 'Hunchback of Notre Dame The (1996)',\n",
       " 'Antz (1998)',\n",
       " 'Fantasia (1940)',\n",
       " 'Nightmare Before Christmas The (1993)',\n",
       " 'Galaxy Quest (1999)',\n",
       " 'Jungle Book The (1967)',\n",
       " 'Clueless (1995)',\n",
       " 'Wrong Trousers The (1993)',\n",
       " 'My Cousin Vinny (1992)',\n",
       " 'South Park: Bigger Longer and Uncut (1999)',\n",
       " 'Addams Family The (1991)',\n",
       " 'Sleeping Beauty (1959)',\n",
       " 'Peter Pan (1953)',\n",
       " 'Close Shave A (1995)']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_movies = [\"Toy Story (1995)\",\n",
    "               \"Toy Story 2 (1999)\",\n",
    "               \"Little Nemo: Adventures in Slumberland (1992)\",\n",
    "               \"It Takes Two (1995)\",\n",
    "               \"Mighty Aphrodite (1995)\",\n",
    "               \"Ghostbusters (1984)\",\n",
    "               \"Ace Ventura: Pet Detective (1994)\"]\n",
    "top_movie = predict(list_movies, model, movie_to_idx, idx_to_movie)\n",
    "top_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98112845",
   "metadata": {},
   "source": [
    "Nous pouvons voir que le modèle fait des recommandations intéressantes dans le genre Aventure/Fantaisie. Notez que le modèle n'a pas accès au genre des films.\n",
    "\n",
    "Dans ce cas, le modèle a pu suggérer d'excellents films, comme Aladdin ou Star Wars, qui sont en adéquation avec le thème de l'historique de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea55b42",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f21f8e",
   "metadata": {},
   "source": [
    "Dans ce projet, nous avons developpe un systeme de recommendation de films base sur le traitement du langage naturel a partir des titres des films et des historiques des utilisateurs. Alors que les performances du modele durant l'entrainement n'etaient pas bonnes en terme de precision, les recommendations du systeme faisaient du sens lors du test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
